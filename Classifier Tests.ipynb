{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fesGq3FHH7x7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, fbeta_score, accuracy_score, f1_score, make_scorer\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold, cross_validate\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('cases_2021_train_processed_2.csv')\n",
        "test = pd.read_csv('cases_2021_test_processed_unlabelled_2.csv')\n",
        "\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.max_columns', 1000)"
      ],
      "metadata": {
        "id": "AZ0ld-gAIP6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>### PART 1.1 FEATURE SELECTION ### </h3>"
      ],
      "metadata": {
        "id": "KlwhpkJyIUvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Country and province probably aren't needed\n",
        "# That information is implied in the other data (incident rate, confirmed, etc.)\n",
        "# There are too many different countries to distinguish all as levels\n",
        "# AND some are present in train but not test and vice versa\n",
        "\n",
        "train = train.drop(columns=['province', 'country'])\n",
        "test = test.drop(columns=['province', 'country'])"
      ],
      "metadata": {
        "id": "S1T7_B9gIT-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclude latitude and longitude\n",
        "# Once again this is somewhat implicit in the other values\n",
        "\n",
        "train = train.drop(columns=['latitude', 'longitude'])\n",
        "test = test.drop(columns=['latitude', 'longitude'])"
      ],
      "metadata": {
        "id": "g7_H9hnQId8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't want to use any of Confirmed, Deaths, Recovered, or Active in their og form\n",
        "# There is such a range in values that it will mess with results\n",
        "# Ideally we can capture the same info by converting to ratios\n",
        "# PLUS we have less variables this way\n",
        "# Case_Fatality_Ratio = Deaths / Confirmed\n",
        "# Incident_Rate = Not sure how exactly it's derived\n",
        "# (something with population and cases I think)\n",
        "### OUR OWN ###\n",
        "# Case_Active_Ratio = Active / Confirmed\n",
        "# We can scrap Recovered\n",
        "# It's going to be super correlated with Case_Fatality_Ratio and Case_Active_Ratio\n",
        "\n",
        "train['Case_Active_Ratio'] = train['Active'] / train['Confirmed'] * 100\n",
        "train = train.drop(columns=['Confirmed', 'Deaths', 'Recovered', 'Active'])\n",
        "\n",
        "test['Case_Active_Ratio'] = test['Active'] / test['Confirmed'] * 100\n",
        "test = test.drop(columns=['Confirmed', 'Deaths', 'Recovered', 'Active'])"
      ],
      "metadata": {
        "id": "3pm7eulhIfRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>### PART 1.2 MAPPING THE FEATURES ###</h3>"
      ],
      "metadata": {
        "id": "slPRKaoeIhQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping outcome_group\n",
        "# dec = 0\n",
        "# hosp = 1\n",
        "# non-hosp = 2\n",
        "train.outcome_group = pd.Categorical(train.outcome_group)\n",
        "train.outcome_group = train.outcome_group.cat.codes"
      ],
      "metadata": {
        "id": "cSaBhv5nIoLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping sex\n",
        "# 0 = Female\n",
        "# 1 = Male\n",
        "train.sex = pd.Categorical(train.sex)\n",
        "train.sex = train.sex.cat.codes\n",
        "\n",
        "test.sex = pd.Categorical(test.sex)\n",
        "test.sex = test.sex.cat.codes"
      ],
      "metadata": {
        "id": "S26b79XuIqvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping chronic_disease_binary\n",
        "# 0 = False\n",
        "# 1 = True\n",
        "train.chronic_disease_binary = pd.Categorical(train.chronic_disease_binary)\n",
        "train.chronic_disease_binary = train.chronic_disease_binary.cat.codes\n",
        "\n",
        "test.chronic_disease_binary = pd.Categorical(test.chronic_disease_binary)\n",
        "test.chronic_disease_binary = test.chronic_disease_binary.cat.codes"
      ],
      "metadata": {
        "id": "e3OJ2mrzIuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping date confirmation\n",
        "# value = month, eg 2020-04-23 = 4\n",
        "train['date_confirmation'] = pd.DatetimeIndex(train['date_confirmation']).month\n",
        "test['date_confirmation'] = pd.DatetimeIndex(test['date_confirmation']).month"
      ],
      "metadata": {
        "id": "k5h1NbUWIw0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>### PART 1.3 Balancing Classes ###"
      ],
      "metadata": {
        "id": "pWr57qRVIzj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can try to use SMOTE oversampling\n",
        "# Idk whether it will help the way it should but it's worth a try\n",
        "\n",
        "# Need to split out our validation set before this\n",
        "# Approx 80/20 train/validation split\n",
        "np.random.seed(459)\n",
        "train_ind = np.random.rand(len(train)) < 0.8\n",
        "\n",
        "validation = train[~train_ind]\n",
        "train = train[train_ind]"
      ],
      "metadata": {
        "id": "LrpfjwvHIyxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we oversample JUST our train set\n",
        "X_train = train.drop(columns=['outcome_group'])\n",
        "Y_train = train['outcome_group']"
      ],
      "metadata": {
        "id": "OHobScPZI-2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state = 459)\n",
        "\n",
        "# Fit the SMOTE\n",
        "X_oversample, Y_oversample = smote.fit_resample(X_train, Y_train)"
      ],
      "metadata": {
        "id": "q1oclUbLJBI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is our final training set\n",
        "train = X_oversample\n",
        "train['outcome_group'] = Y_oversample\n",
        "\n",
        "train.to_csv(\"train.csv\")\n",
        "validation.to_csv(\"validation.csv\")\n",
        "test.to_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "FRCNfdFoJJxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>### PART 1.4.1 KNN ###</h3>"
      ],
      "metadata": {
        "id": "LfrB3A9sJSY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "validation = pd.read_csv('validation.csv')\n",
        "\n",
        "# processing for knn\n",
        "train_knn = train.iloc[:, 1:]\n",
        "test_knn = test.iloc[:, 1:].values\n",
        "cols = validation.columns.tolist()\n",
        "cols = cols[:-2] + cols[-1:] + cols[7:8]\n",
        "validation_knn = validation[cols].iloc[:, 1:]\n",
        "\n",
        "#x_train, y_train\n",
        "X_train_knn = train_knn.iloc[:, :-1].values\n",
        "y_train_knn = train_knn.iloc[:, 7].values\n",
        "#x_val, y_val\n",
        "X_validation_knn = validation_knn.iloc[:, :-1].values\n",
        "y_validation_knn = validation_knn.iloc[:, 7].values"
      ],
      "metadata": {
        "id": "wHXVlK9bJPeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper-para tuning using grid search\n",
        "def custom_scorer(y_validation_knn, y_pred_knn):\n",
        "    return f1_score(y_validation_knn, y_pred_knn, average = None) [0]\n",
        "\n",
        "scoring_knn = {'accuracy' : make_scorer(accuracy_score), \n",
        "               'f1_dec' : make_scorer(custom_scorer),\n",
        "               'f1_score' : make_scorer(f1_score, average='macro')}\n",
        "\n",
        "knn_para = {\n",
        "    \"n_neighbors\": range(3, 13),\n",
        "    \"weights\": [\"uniform\", \"distance\"],\n",
        "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
        "    \"metric\": [\"euclidean\", \"manhattan\"]\n",
        "}\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "kfold = KFold(n_splits=5, random_state=5, shuffle=True)\n",
        "# sfold = StratifiedKFold(n_splits=5, random_state=4, shuffle=True)\n",
        "\n",
        "search = GridSearchCV(estimator=knn, param_grid=knn_para, n_jobs=-1, cv=kfold, scoring=scoring_knn, refit='f1_dec', error_score=0)\n",
        "results = search.fit(X_train_knn, y_train_knn)"
      ],
      "metadata": {
        "id": "BQQwrkUfh5rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rk1k7o19kc97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using best parameters from tuning \n",
        "knn_best = knn.set_params(**results.best_params_)\n",
        "knn_best.fit(X_train_knn, y_train_knn)\n",
        "y_pred_knn = knn_best.predict(X_validation_knn)\n",
        "\n",
        "# generating knn_predict.csv\n",
        "val_id_pred = pd.DataFrame(y_pred_knn, columns = ['KNN_labels'])\n",
        "val_id_pred.to_csv('KNN_predict.csv')\n",
        "\n",
        "# generating knn.txt for all parameter combinations and given metric"
      ],
      "metadata": {
        "id": "dEppKjU3h6B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture cap --no-stderr\n",
        "for param, acc, f1, f1d in zip(results.cv_results_['params'], results.cv_results_['mean_test_accuracy'], results.cv_results_['mean_test_f1_score'], results.cv_results_['mean_test_f1_dec']):\n",
        "    print (\"\\nParameters:\", param, \"\\nMean accuracy\\t   : \", acc, \"\\nMean macro F1 score: \", f1, \"\\nMean F1 deceased   : \", f1d)"
      ],
      "metadata": {
        "id": "OLudalr_h6Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('knn_tuning.txt', 'w') as f:\n",
        "        f.write(cap.stdout)"
      ],
      "metadata": {
        "id": "kq4r_h_Qh6VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performace for best parameters\n",
        "\n",
        "res = cross_validate(knn_best, X_validation_knn, y_validation_knn, cv = kfold, scoring = scoring_knn)\n",
        "\n",
        "print(\"All macro F1 scores across validation data: \")\n",
        "print(*res['test_f1_score'], sep=', ')\n",
        "print(\"\\nAll F1 scores across validation data for 'deceased':\")\n",
        "print(*res['test_f1_dec'], sep=', ')\n",
        "\n",
        "print(\"\\nMean accuracy across val    : {:0.3f}\".format(res['test_accuracy'].mean()))\n",
        "print(\"Mean macro F1 across val    : {:0.3f}\".format(res['test_f1_score'].mean()))\n",
        "print(\"Mean macro F1 for 'deceased': {:0.3f}\".format(res['test_f1_dec'].mean()))"
      ],
      "metadata": {
        "id": "YCfvCe3iiVHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>### PART 1.4.2 SVM ###</h3>"
      ],
      "metadata": {
        "id": "3L3dZNOeJZE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Current Working directory\n",
        "dir = os.getcwd()\n",
        "\n",
        "\n",
        "# Load Data\n",
        "train = pd.read_csv(dir + \"/train.csv\")\n",
        "validation = pd.read_csv(dir + \"/validation.csv\")\n",
        "\n",
        "\n",
        "# Remove column 1 (unnamed column)\n",
        "train = train.iloc[: , 1:]\n",
        "validation = validation.iloc[: , 1:]\n",
        "\n",
        "# Split Labeled Data\n",
        "train_y = train['outcome_group']\n",
        "train_x = train.drop(columns = 'outcome_group')\n",
        "\n",
        "validation_y = validation['outcome_group']\n",
        "validation_x = validation.drop(columns = 'outcome_group')\n",
        "\n",
        "kfold = KFold(n_splits=5)\n",
        "\n",
        "def deceased_f1(truth, pred):\n",
        "    return f1_score(truth, pred, average=None)[0]\n",
        "\n",
        "scorers = {\n",
        "    'f1_score': make_scorer(fbeta_score, beta=1, average='macro'),\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'f1_micro': make_scorer(deceased_f1),\n",
        "}\n",
        "\n",
        "# kernels = ['rbf', 'sigmoid','linear']\n",
        "# C_range = [0.1, 1, 2, 3, 4, 5, 6, 7, 10]\n",
        "# gamma_range = [2, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "\n",
        "# Parameter for the best results\n",
        "param_grid = {'C': [6],\n",
        "              'gamma': [.5],\n",
        "              'kernel': ['rbf']\n",
        "              }\n",
        "\n",
        "grid = GridSearchCV(svm.SVC(), param_grid, scoring=scorers, cv = 5, refit = \"accuracy\")\n",
        "\n",
        "# Fitting the model\n",
        "grid.fit(validation_x, validation_y)"
      ],
      "metadata": {
        "id": "ODu_I0SMJdmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See results for SVM\n",
        "results = cross_validate(estimator=grid,\n",
        "                            X=X_valid,\n",
        "                            y=y_valid,\n",
        "                            cv=kfold,\n",
        "                            scoring=scorers)\n",
        "\n",
        "print(\"Macro F1-score:\", results['test_f1_score'])\n",
        "print(\"Overall accuracy:\", results['test_accuracy'])\n",
        "print(\"F1-score on deceased:\", results['test_f1_micro'])\n",
        "\n",
        "print(\"Mean macro F1-score:\", np.mean(results['test_f1_score']))\n",
        "print(\"Mean F1-score on deceased:\", np.mean(results['test_f1_micro']))\n",
        "print(\"Mean overall accuracy:\", np.mean(results['test_accuracy']))"
      ],
      "metadata": {
        "id": "buGjDcjYlSTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions for validation set\n",
        "grid_predictions = grid.predict(X_valid)\n",
        "\n",
        "pd.DataFrame(grid_predictions, columns= ['SVM_labels']).to_csv(\"SVM_predictions.csv\")"
      ],
      "metadata": {
        "id": "cWvwsIQFlZ4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>### PART 1.4.3 XGB ###</h3>"
      ],
      "metadata": {
        "id": "siQBBLRMJeGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = get_X_y(train)\n",
        "X_valid, y_valid = get_X_y(validation)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(objective='multi:softmax', use_label_encoder=False, eval_metric=['merror'])\n",
        "\n",
        "xgb.set_config(verbosity=1)"
      ],
      "metadata": {
        "id": "hCjCECNhJkYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deceased_f1(truth, pred):\n",
        "    return f1_score(truth, pred, average=None)[0]\n",
        "\n",
        "scorers = {\n",
        "    'f1_score': make_scorer(fbeta_score, beta=1, average='macro'),\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'f1_micro': make_scorer(deceased_f1)\n",
        "}\n",
        "\n",
        "# Params were altered across multiple grid search runs\n",
        "# As described in report\n",
        "params = {\n",
        "    'min_child_weight': [1],\n",
        "    'gamma': [0.5],\n",
        "    'subsample': [1],\n",
        "    'colsample_bytree': [0.4],\n",
        "    'n_estimators' : [1500],\n",
        "    'max_depth': [5],\n",
        "    'learning_rate': [0.005]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(xgb_model,\n",
        "                    param_grid = params,\n",
        "                    n_jobs = -1,\n",
        "                    scoring=scorers,\n",
        "                    cv = 5,\n",
        "                    refit = \"accuracy\")"
      ],
      "metadata": {
        "id": "YWHhUI_yJsoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = xgb.XGBClassifier(silent=False,\n",
        "                               min_child_weight=1,\n",
        "                               gamma=0.5,\n",
        "                               subsample=1,\n",
        "                               colsample_bytree=0.4,\n",
        "                               n_estimators=1500,\n",
        "                               max_depth=5,\n",
        "                               learning_rate=0.005,\n",
        "                               # early_stopping_rounds=10,\n",
        "                               objective='multi:softmax',\n",
        "                               nthread=4)\n",
        "\n",
        "eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
        "# eval_metric = [\"auc\",\"merror\"]\n",
        "classifier.fit(X_train, y_train, eval_set=eval_set, eval_metric=['merror'])\n",
        "\n",
        "### Performance Check ###\n",
        "print('### Train ###')\n",
        "print_performance_xgb(train, classifier)\n",
        "print('### Validation ###')\n",
        "print_performance_xgb(validation, classifier)\n",
        "\n",
        "train1 = classifier.predict_proba(X_train)\n",
        "valid1 = classifier.predict_proba(validation.loc[:, ~validation.columns.isin(['outcome_group'])])\n",
        "\n",
        "print(np.asarray(y_train))\n",
        "print(np.argmax(train1, axis=1))\n",
        "\n",
        "y_true = list(y_train)\n",
        "y_pred = list(np.argmax(train1, axis=1))\n",
        "\n",
        "print(fbeta_score(y_true, y_pred, beta=1.0, average='macro'))\n",
        "\n",
        "y_true = list(validation['outcome_group'])\n",
        "y_pred = list(np.argmax(valid1, axis=1))\n",
        "\n",
        "print(fbeta_score(y_true, y_pred, beta=1.0, average='macro'))"
      ],
      "metadata": {
        "id": "RebEIejtJu43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>### PART 1.5 Overfitting ###</h3>"
      ],
      "metadata": {
        "id": "RbC8G0SaKB6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-tRjXtUCJ7nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>### PART 1.6 Comparative Study ###</h3>"
      ],
      "metadata": {
        "id": "bAAya4zLKC5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y(data):\n",
        "    return data.loc[:, ~data.columns.isin(['outcome_group'])], list(data['outcome_group'])\n",
        "\n",
        "# Used to see how we performed on each class\n",
        "def print_performance(truth, pred):\n",
        "    yt = truth\n",
        "    yp = pred\n",
        "\n",
        "    f1_s = fbeta_score(yt, yp, beta=1.0, average='macro')\n",
        "    print(\"F1 Score: \", f1_s)\n",
        "\n",
        "    print(\"Accuracy: \", sum(yt == yp)/len(yt))\n",
        "\n",
        "    np_y = np.vstack((yt, yp)).T\n",
        "\n",
        "    class_0 = np_y[np_y[:,0] == 0]\n",
        "    class_1 = np_y[np_y[:,0] == 1]\n",
        "    class_2 = np_y[np_y[:,0] == 2]\n",
        "\n",
        "    print(\"Detection Rate Class 0 (Deceased): \", np.sum(class_0[:,0] == class_0[:,1])/class_0.shape[0])\n",
        "    print(\"Detection Rate Class 1 (Hospitalized): \", np.sum(class_1[:, 0] == class_1[:, 1])/class_1.shape[0])\n",
        "    print(\"Detection Rate Class 2 (Non-Hospitalized): \", np.sum(class_2[:, 0] == class_2[:, 1])/class_2.shape[0])\n",
        "    print(\"By class macro F1: \", f1_score(yt, yp, average=None))\n",
        "\n",
        "\n",
        "svm = pd.read_csv('SVM_predictions.csv')['SVM_labels']\n",
        "print(svm)\n",
        "\n",
        "knn = pd.read_csv('KNN_predict.csv')['KNN_labels']\n",
        "print(knn)\n",
        "\n",
        "xgb = pd.read_csv('xgb_predictions.csv')['Prediction']\n",
        "print(xgb)\n",
        "\n",
        "truth = pd.read_csv('validation.csv')['outcome_group']\n",
        "print(truth)\n",
        "\n",
        "print(\"### KNN PERFORMANCE ###\")\n",
        "print_performance(truth, knn)\n",
        "print(\"\\n### SVM PERFORMANCE ###\")\n",
        "print_performance(truth, svm)\n",
        "print(\"\\n### XGB PERFORMANCE ###\")\n",
        "print_performance(truth, xgb)"
      ],
      "metadata": {
        "id": "NCuiy6RjKIg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>### PART 1.7 Predictions on Test ###</h3>"
      ],
      "metadata": {
        "id": "tGL8HhKEKI7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = list(np.argmax(classifier.predict_proba(test), axis=1))\n",
        "test_preds = pd.DataFrame(test_preds, columns=['Prediction'])\n",
        "test_preds['Id'] = test_preds.index\n",
        "test_preds.to_csv('predictions.csv', index=False)\n",
        "\n",
        "print(test_preds['Prediction'].value_counts())"
      ],
      "metadata": {
        "id": "_y7EOqpzKPun"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}